#!/bin/bash
set -e

echo "==================================================="
echo "ğŸš€ ComfyUI WAN RunPod Template Starting..."
echo "==================================================="

# â”€â”€ ç’°å¢ƒå¤‰æ•°è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export PYTHONUNBUFFERED=1
export HF_HOME="/tmp/huggingface"
export COMFYUI_MODEL_PATH="/ComfyUI/models"

# Initialize models directory if using persistent storage
if [ -n "$RUNPOD_VOLUME_PATH" ] && [ -d "$RUNPOD_VOLUME_PATH" ]; then
    echo "[INFO] Using persistent storage: $RUNPOD_VOLUME_PATH"
    MODELS_PATH="$RUNPOD_VOLUME_PATH/models"
    mkdir -p $MODELS_PATH/{unet,clip,vae,loras}
    
    # Copy built-in models to persistent storage if they don't exist
    if [ ! -f "$MODELS_PATH/unet/wan2.1-t2v-14b-Q4_K_S.gguf" ]; then
        echo "[INFO] Copying WAN models to persistent storage..."
        cp -r $COMFYUI_MODEL_PATH/* $MODELS_PATH/
        echo "[INFO] WAN models copied to persistent storage"
    fi
    
    # Create symlinks to persistent storage
    rm -rf $COMFYUI_MODEL_PATH
    ln -sf $MODELS_PATH $COMFYUI_MODEL_PATH
    echo "[INFO] Models linked to persistent storage"
else
    echo "[INFO] Using built-in models (models are pre-installed in image)"
fi

# â”€â”€ äº‹å‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç¢ºèª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "[INFO] Checking WAN model files..."

# Check if WAN models exist
if [ -f "$COMFYUI_MODEL_PATH/unet/wan2.1-t2v-14b-Q4_K_S.gguf" ]; then
    echo "[INFO] âœ“ WAN T2V model found"
else
    echo "[ERROR] WAN T2V model not found!"
fi

if [ -f "$COMFYUI_MODEL_PATH/clip/umt5-xxl-encoder-Q5_K_S.gguf" ]; then
    echo "[INFO] âœ“ UMT5 encoder found"
else
    echo "[ERROR] UMT5 encoder not found!"
fi

if [ -f "$COMFYUI_MODEL_PATH/vae/wan_2.1_vae.safetensors" ]; then
    echo "[INFO] âœ“ WAN VAE found"
else
    echo "[ERROR] WAN VAE not found!"
fi

if [ -f "$COMFYUI_MODEL_PATH/loras/Wan21_T2V_14B_lightx2v_cfg_step_distill_lora_rank32.safetensors" ]; then
    echo "[INFO] âœ“ WAN LoRA found"
else
    echo "[ERROR] WAN LoRA not found!"
fi

# â”€â”€ ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ¼ãƒ‰ç¢ºèª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "[INFO] Checking custom nodes..."

# Check if custom nodes exist
if [ -d "/ComfyUI/custom_nodes/ComfyUI-Manager" ]; then
    echo "[INFO] âœ“ ComfyUI-Manager found"
else
    echo "[ERROR] ComfyUI-Manager not found!"
fi

if [ -d "/ComfyUI/custom_nodes/ComfyUI-GGUF" ]; then
    echo "[INFO] âœ“ ComfyUI-GGUF found"
else
    echo "[ERROR] ComfyUI-GGUF not found!"
fi

cd /ComfyUI

# â”€â”€ Status summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo ""
echo "ğŸ“Š Setup Summary:"
echo "   Custom nodes: $(ls -d /ComfyUI/custom_nodes/*/ 2>/dev/null | wc -l) latest versions installed"
echo "   Models: $(find $COMFYUI_MODEL_PATH -name "*.gguf" -o -name "*.safetensors" 2>/dev/null | wc -l) files ready"
echo "   CUDA available: $(python3 -c 'import torch; print(torch.cuda.is_available())' 2>/dev/null || echo 'Unknown')"
echo ""

# â”€â”€ Jupyter Lab ã®èµ·å‹• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "ğŸ”¬ Starting Jupyter Lab..."
mkdir -p /workspace/notebooks
cd /workspace

# Jupyter Lab ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•
nohup jupyter lab \
    --ip=0.0.0.0 \
    --port=8888 \
    --no-browser \
    --allow-root \
    --NotebookApp.token='' \
    --NotebookApp.password='' \
    --NotebookApp.allow_origin='*' \
    --NotebookApp.disable_check_xsrf=True \
    > /tmp/jupyter.log 2>&1 &

echo "[INFO] Jupyter Lab started on port 8888"

# â”€â”€ ComfyUIã®èµ·å‹• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo "ğŸ¨ Starting ComfyUI..."
cd /ComfyUI

# â”€â”€ Provide stub for torch_directml on non-DirectML environments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python3 -c "
import importlib, sys, types
try:
    import torch
    # If torch_directml is unavailable (e.g., CUDA server) create a safe stub
    try:
        importlib.import_module('torch_directml')
    except ModuleNotFoundError:
        stub = types.ModuleType('torch_directml')
        # Return a CUDA device if available, else CPU â€“ enough for ComfyUI to proceed
        stub.device = lambda *args, **kwargs: torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        sys.modules['torch_directml'] = stub
        print('[INFO] torch_directml stub created for CUDA environment')
except Exception as e:
    print(f'[WARNING] torch_directml stub creation failed: {e}')
"

# ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–¢æ•°
cleanup() {
    echo "[INFO] Shutting down services..."
    pkill -f "jupyter lab" 2>/dev/null || true
    pkill -f "python main.py" 2>/dev/null || true
    exit 0
}

# ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¨­å®š
trap cleanup SIGTERM SIGINT

echo "[INFO] ComfyUI and Jupyter Lab are now running:"
echo "  - ComfyUI: http://localhost:6006"
echo "  - Jupyter Lab: http://localhost:8888"
echo ""
echo "ğŸ¬ Ready to generate videos with WAN!"

# ComfyUI ã‚’ãƒ•ã‚©ã‚¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•ï¼ˆãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ï¼‰
python main.py \
    --listen 0.0.0.0 \
    --port 6006 \
    --preview-method auto \
    --use-split-cross-attention